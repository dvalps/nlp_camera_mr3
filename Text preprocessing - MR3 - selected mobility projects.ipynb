{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing: MR3\n",
    "\n",
    "Creating clean text for LDA topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d81eec7566e4fc58aba5e9acec4b6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from tqdm import tqdm_notebook #for loops showing progress meter\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29314, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the fields for NLP analysis are merge in the data frame h2020_full\n",
    "h2020 = pd.read_csv(\"datasets/data_may20/h2020_clean/h2020_full.csv\")\n",
    "h2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rcn</th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>status</th>\n",
       "      <th>programme</th>\n",
       "      <th>topics</th>\n",
       "      <th>frameworkProgramme</th>\n",
       "      <th>title</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>...</th>\n",
       "      <th>rcn_report</th>\n",
       "      <th>title_report</th>\n",
       "      <th>summary</th>\n",
       "      <th>workPerformed</th>\n",
       "      <th>finalResults</th>\n",
       "      <th>lastUpdateDate</th>\n",
       "      <th>projectID</th>\n",
       "      <th>projectAcronym</th>\n",
       "      <th>relatedFile</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229267</td>\n",
       "      <td>894593</td>\n",
       "      <td>ICARUS</td>\n",
       "      <td>SIGNED</td>\n",
       "      <td>H2020-EU.3.4.7.</td>\n",
       "      <td>SESAR-ER4-31-2019</td>\n",
       "      <td>H2020</td>\n",
       "      <td>INTEGRATED COMMON ALTITUDE REFERENCE SYSTEM FO...</td>\n",
       "      <td>2020-05-01</td>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>229284</td>\n",
       "      <td>897004</td>\n",
       "      <td>ISLand</td>\n",
       "      <td>SIGNED</td>\n",
       "      <td>H2020-EU.1.3.2.</td>\n",
       "      <td>MSCA-IF-2019</td>\n",
       "      <td>H2020</td>\n",
       "      <td>Isolation and Segregation Landscape. Archaeolo...</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229281</td>\n",
       "      <td>896300</td>\n",
       "      <td>STRETCH</td>\n",
       "      <td>SIGNED</td>\n",
       "      <td>H2020-EU.1.3.2.</td>\n",
       "      <td>MSCA-IF-2019</td>\n",
       "      <td>H2020</td>\n",
       "      <td>Smart Textiles for RETrofitting and Monitoring...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229265</td>\n",
       "      <td>892890</td>\n",
       "      <td>RhythmicPrediction</td>\n",
       "      <td>SIGNED</td>\n",
       "      <td>H2020-EU.1.3.2.</td>\n",
       "      <td>MSCA-IF-2019</td>\n",
       "      <td>H2020</td>\n",
       "      <td>Rhythmic prediction in speech perception: are ...</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229235</td>\n",
       "      <td>886828</td>\n",
       "      <td>ASAP</td>\n",
       "      <td>SIGNED</td>\n",
       "      <td>H2020-EU.1.3.2.</td>\n",
       "      <td>MSCA-IF-2019</td>\n",
       "      <td>H2020</td>\n",
       "      <td>Advanced Solutions for Asphalt Pavements</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rcn      id             acronym  status        programme  \\\n",
       "0  229267  894593              ICARUS  SIGNED  H2020-EU.3.4.7.   \n",
       "1  229284  897004              ISLand  SIGNED  H2020-EU.1.3.2.   \n",
       "2  229281  896300             STRETCH  SIGNED  H2020-EU.1.3.2.   \n",
       "3  229265  892890  RhythmicPrediction  SIGNED  H2020-EU.1.3.2.   \n",
       "4  229235  886828                ASAP  SIGNED  H2020-EU.1.3.2.   \n",
       "\n",
       "              topics frameworkProgramme  \\\n",
       "0  SESAR-ER4-31-2019              H2020   \n",
       "1       MSCA-IF-2019              H2020   \n",
       "2       MSCA-IF-2019              H2020   \n",
       "3       MSCA-IF-2019              H2020   \n",
       "4       MSCA-IF-2019              H2020   \n",
       "\n",
       "                                               title   startDate     endDate  \\\n",
       "0  INTEGRATED COMMON ALTITUDE REFERENCE SYSTEM FO...  2020-05-01  2022-07-31   \n",
       "1  Isolation and Segregation Landscape. Archaeolo...  2020-11-01  2023-10-31   \n",
       "2  Smart Textiles for RETrofitting and Monitoring...  2020-09-01  2022-08-31   \n",
       "3  Rhythmic prediction in speech perception: are ...  2021-01-01  2022-12-31   \n",
       "4           Advanced Solutions for Asphalt Pavements  2021-09-01  2023-08-31   \n",
       "\n",
       "   ... rcn_report title_report  summary  workPerformed finalResults  \\\n",
       "0  ...        NaN          NaN      NaN            NaN          NaN   \n",
       "1  ...        NaN          NaN      NaN            NaN          NaN   \n",
       "2  ...        NaN          NaN      NaN            NaN          NaN   \n",
       "3  ...        NaN          NaN      NaN            NaN          NaN   \n",
       "4  ...        NaN          NaN      NaN            NaN          NaN   \n",
       "\n",
       "  lastUpdateDate projectID projectAcronym relatedFile  url  \n",
       "0            NaN       NaN            NaN         NaN  NaN  \n",
       "1            NaN       NaN            NaN         NaN  NaN  \n",
       "2            NaN       NaN            NaN         NaN  NaN  \n",
       "3            NaN       NaN            NaN         NaN  NaN  \n",
       "4            NaN       NaN            NaN         NaN  NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h2020.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL IN!\n",
    "\n",
    "MISSING VALUES (_____ projects):\n",
    "- title: 0\n",
    "- objective: 0\n",
    "- summary: 12 378 (48%)\n",
    "- workPerformed: 12 377 (48%)\n",
    "- finalResults: 12 388 (48%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25716, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the fields for NLP analysis are merge in the data frame fp7_full\n",
    "fp7 = pd.read_csv(\"datasets/data_may20/fp7_clean/fp7_full.csv\")\n",
    "fp7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rcn</th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>status</th>\n",
       "      <th>programme</th>\n",
       "      <th>topics</th>\n",
       "      <th>frameworkProgramme</th>\n",
       "      <th>title</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>...</th>\n",
       "      <th>coordinator</th>\n",
       "      <th>coordinatorCountry</th>\n",
       "      <th>participants</th>\n",
       "      <th>participantCountries</th>\n",
       "      <th>subjects</th>\n",
       "      <th>summary</th>\n",
       "      <th>lastUpdateDate</th>\n",
       "      <th>rcn_report</th>\n",
       "      <th>title_report</th>\n",
       "      <th>projectAcronym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203726</td>\n",
       "      <td>115760</td>\n",
       "      <td>ZAPI</td>\n",
       "      <td>ONG</td>\n",
       "      <td>FP7-JTI</td>\n",
       "      <td>IMI-JU-11-2013-04</td>\n",
       "      <td>FP7</td>\n",
       "      <td>Zoonotic Anticipation and Preparedness Initiative</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>2020-02-29</td>\n",
       "      <td>...</td>\n",
       "      <td>MERIAL SAS</td>\n",
       "      <td>FR</td>\n",
       "      <td>ACADEMISCH ZIEKENHUIS LEIDEN;STIFTUNG TIERAERZ...</td>\n",
       "      <td>NL;DE;SE;FR;ES</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>109748</td>\n",
       "      <td>320377</td>\n",
       "      <td>NetSat</td>\n",
       "      <td>ONG</td>\n",
       "      <td>FP7-IDEAS-ERC</td>\n",
       "      <td>ERC-AG-PE8</td>\n",
       "      <td>FP7</td>\n",
       "      <td>Networked Pico-Satellite Distributed System Co...</td>\n",
       "      <td>2014-08-01</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Zentrum fuer Telematik e.V.</td>\n",
       "      <td>DE</td>\n",
       "      <td>JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG</td>\n",
       "      <td>DE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Multi-satellite systems are currently gaining ...</td>\n",
       "      <td>2017-10-30 17:19:45</td>\n",
       "      <td>202963.0</td>\n",
       "      <td>Periodic Report Summary 2 - NETSAT (Networked ...</td>\n",
       "      <td>NetSat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188675</td>\n",
       "      <td>615785</td>\n",
       "      <td>EMERGING SUBJECTS</td>\n",
       "      <td>ONG</td>\n",
       "      <td>FP7-IDEAS-ERC</td>\n",
       "      <td>ERC-CG-2013-SH2</td>\n",
       "      <td>FP7</td>\n",
       "      <td>Emerging Subjects of the New Economy: Tracing ...</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>...</td>\n",
       "      <td>UNIVERSITY COLLEGE LONDON</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emerging Subjects has reached a mid-point in i...</td>\n",
       "      <td>2018-01-15 17:25:32</td>\n",
       "      <td>213801.0</td>\n",
       "      <td>Periodic Report Summary 2 - EMERGING SUBJECTS ...</td>\n",
       "      <td>EMERGING SUBJECTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>188672</td>\n",
       "      <td>615640</td>\n",
       "      <td>FOREFRONT</td>\n",
       "      <td>ONG</td>\n",
       "      <td>FP7-IDEAS-ERC</td>\n",
       "      <td>ERC-CG-2013-PE6</td>\n",
       "      <td>FP7</td>\n",
       "      <td>Frontiers of Extended Formulations</td>\n",
       "      <td>2014-09-01</td>\n",
       "      <td>2019-08-31</td>\n",
       "      <td>...</td>\n",
       "      <td>UNIVERSITE LIBRE DE BRUXELLES</td>\n",
       "      <td>BE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The FOREFRONT project (Frontiers of Extended F...</td>\n",
       "      <td>2017-07-24 18:20:56</td>\n",
       "      <td>201439.0</td>\n",
       "      <td>Periodic Report Summary 2 - FOREFRONT (Frontie...</td>\n",
       "      <td>FOREFRONT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189842</td>\n",
       "      <td>617196</td>\n",
       "      <td>CORRELMAT</td>\n",
       "      <td>ONG</td>\n",
       "      <td>FP7-IDEAS-ERC</td>\n",
       "      <td>ERC-CG-2013-PE3</td>\n",
       "      <td>FP7</td>\n",
       "      <td>Predictive electronic structure calculations f...</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>...</td>\n",
       "      <td>ECOLE POLYTECHNIQUE</td>\n",
       "      <td>FR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“Correlated electron materials”, i.e. compound...</td>\n",
       "      <td>2017-05-16 10:26:14</td>\n",
       "      <td>197668.0</td>\n",
       "      <td>Mid-Term Report Summary - CORRELMAT (Predictiv...</td>\n",
       "      <td>CORRELMAT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      rcn      id            acronym status      programme             topics  \\\n",
       "0  203726  115760               ZAPI    ONG        FP7-JTI  IMI-JU-11-2013-04   \n",
       "1  109748  320377             NetSat    ONG  FP7-IDEAS-ERC         ERC-AG-PE8   \n",
       "2  188675  615785  EMERGING SUBJECTS    ONG  FP7-IDEAS-ERC    ERC-CG-2013-SH2   \n",
       "3  188672  615640          FOREFRONT    ONG  FP7-IDEAS-ERC    ERC-CG-2013-PE6   \n",
       "4  189842  617196          CORRELMAT    ONG  FP7-IDEAS-ERC    ERC-CG-2013-PE3   \n",
       "\n",
       "  frameworkProgramme                                              title  \\\n",
       "0                FP7  Zoonotic Anticipation and Preparedness Initiative   \n",
       "1                FP7  Networked Pico-Satellite Distributed System Co...   \n",
       "2                FP7  Emerging Subjects of the New Economy: Tracing ...   \n",
       "3                FP7                 Frontiers of Extended Formulations   \n",
       "4                FP7  Predictive electronic structure calculations f...   \n",
       "\n",
       "    startDate     endDate  ...                    coordinator  \\\n",
       "0  2015-03-01  2020-02-29  ...                     MERIAL SAS   \n",
       "1  2014-08-01  2019-07-31  ...    Zentrum fuer Telematik e.V.   \n",
       "2  2014-09-01  2019-06-30  ...      UNIVERSITY COLLEGE LONDON   \n",
       "3  2014-09-01  2019-08-31  ...  UNIVERSITE LIBRE DE BRUXELLES   \n",
       "4  2014-07-01  2019-06-30  ...            ECOLE POLYTECHNIQUE   \n",
       "\n",
       "  coordinatorCountry                                       participants  \\\n",
       "0                 FR  ACADEMISCH ZIEKENHUIS LEIDEN;STIFTUNG TIERAERZ...   \n",
       "1                 DE            JULIUS-MAXIMILIANS-UNIVERSITAT WURZBURG   \n",
       "2                 UK                                                NaN   \n",
       "3                 BE                                                NaN   \n",
       "4                 FR                                                NaN   \n",
       "\n",
       "  participantCountries subjects  \\\n",
       "0       NL;DE;SE;FR;ES      NaN   \n",
       "1                   DE      NaN   \n",
       "2                  NaN      NaN   \n",
       "3                  NaN      NaN   \n",
       "4                  NaN      NaN   \n",
       "\n",
       "                                             summary       lastUpdateDate  \\\n",
       "0                                                NaN                  NaN   \n",
       "1  Multi-satellite systems are currently gaining ...  2017-10-30 17:19:45   \n",
       "2  Emerging Subjects has reached a mid-point in i...  2018-01-15 17:25:32   \n",
       "3  The FOREFRONT project (Frontiers of Extended F...  2017-07-24 18:20:56   \n",
       "4  “Correlated electron materials”, i.e. compound...  2017-05-16 10:26:14   \n",
       "\n",
       "  rcn_report                                       title_report  \\\n",
       "0        NaN                                                NaN   \n",
       "1   202963.0  Periodic Report Summary 2 - NETSAT (Networked ...   \n",
       "2   213801.0  Periodic Report Summary 2 - EMERGING SUBJECTS ...   \n",
       "3   201439.0  Periodic Report Summary 2 - FOREFRONT (Frontie...   \n",
       "4   197668.0  Mid-Term Report Summary - CORRELMAT (Predictiv...   \n",
       "\n",
       "      projectAcronym  \n",
       "0                NaN  \n",
       "1             NetSat  \n",
       "2  EMERGING SUBJECTS  \n",
       "3          FOREFRONT  \n",
       "4          CORRELMAT  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp7.profile_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILL IN!\n",
    "\n",
    "MISSING VALUES (______ projects):\n",
    "- title: 0\n",
    "- objective: 0\n",
    "- summary: 4 538 (18%)\n",
    "- workPerformed:  NOT INCLUDED\n",
    "- finalResults:  NOT INCLUDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVIEW IF IT STILL HOLDS FOR MR3\n",
    "\n",
    "In summary, fields:\n",
    "- title and objective are always present and can be used\n",
    "- summary is present in around half of H2020 projects and 72% FP7 projects\n",
    "- workPerformed and finalResults are present in around half of H2020 projecsts and not at all in FP7 projects\n",
    "\n",
    "At the moment, the text for the NLP analysis is going to be constructed by using the biggest constructible string for each project. That will create textual descriptions of projects unequal in length, with some projects having more detailed descriptions than others. If noticed that affects the performance, skewing it more towards one group of projects, the algorithm can be run separately on two or more different groups of projects.\n",
    "Example:\n",
    "- projects with small text. descriptions (just title and objective)\n",
    "- projects with larger text. descriptions (other fields available in addition to title and objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected mobility projects (final, from MR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(926, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobility_projects = pd.read_csv(\"datasets/data_may20/final_project_selection_mr2_outlier_removed.csv\")\n",
    "mobility_projects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>title</th>\n",
       "      <th>objective</th>\n",
       "      <th>summary</th>\n",
       "      <th>workPerformed</th>\n",
       "      <th>finalResults</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>score</th>\n",
       "      <th>framework</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211625</td>\n",
       "      <td>ASSET</td>\n",
       "      <td>ASSET – Aeronautic Study on Seamless Transport</td>\n",
       "      <td>Airport ground processes still conceal a consi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASSET – Aeronautic Study on Seamless Transport...</td>\n",
       "      <td>asset aeronautic study seamless transport airp...</td>\n",
       "      <td>0.572112</td>\n",
       "      <td>FP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>211723</td>\n",
       "      <td>MEFISTO</td>\n",
       "      <td>Methodology for framework programmes’ impact a...</td>\n",
       "      <td>This proposal is responding to the first FP7- ...</td>\n",
       "      <td>The MEFISTO project had three main objectives:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Methodology for framework programmes’ impact a...</td>\n",
       "      <td>methodology framework programme impact assessm...</td>\n",
       "      <td>0.124785</td>\n",
       "      <td>FP7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index      id  acronym  \\\n",
       "0           0      0  211625    ASSET   \n",
       "1           1      1  211723  MEFISTO   \n",
       "\n",
       "                                               title  \\\n",
       "0     ASSET – Aeronautic Study on Seamless Transport   \n",
       "1  Methodology for framework programmes’ impact a...   \n",
       "\n",
       "                                           objective  \\\n",
       "0  Airport ground processes still conceal a consi...   \n",
       "1  This proposal is responding to the first FP7- ...   \n",
       "\n",
       "                                             summary workPerformed  \\\n",
       "0                                                NaN           NaN   \n",
       "1  The MEFISTO project had three main objectives:...           NaN   \n",
       "\n",
       "  finalResults                                               text  \\\n",
       "0          NaN  ASSET – Aeronautic Study on Seamless Transport...   \n",
       "1          NaN  Methodology for framework programmes’ impact a...   \n",
       "\n",
       "                                          clean_text     score framework  \n",
       "0  asset aeronautic study seamless transport airp...  0.572112       FP7  \n",
       "1  methodology framework programme impact assessm...  0.124785       FP7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobility_projects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobility_projects_ids_list = list(mobility_projects.id)\n",
    "len(mobility_projects_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should we use the full dataset or just the mobility projects\n",
    "\n",
    "use_full_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>title</th>\n",
       "      <th>objective</th>\n",
       "      <th>summary</th>\n",
       "      <th>workPerformed</th>\n",
       "      <th>finalResults</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>score</th>\n",
       "      <th>framework</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211625</td>\n",
       "      <td>ASSET</td>\n",
       "      <td>ASSET – Aeronautic Study on Seamless Transport</td>\n",
       "      <td>Airport ground processes still conceal a consi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ASSET – Aeronautic Study on Seamless Transport...</td>\n",
       "      <td>asset aeronautic study seamless transport airp...</td>\n",
       "      <td>0.572112</td>\n",
       "      <td>FP7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index      id acronym  \\\n",
       "0           0      0  211625   ASSET   \n",
       "\n",
       "                                            title  \\\n",
       "0  ASSET – Aeronautic Study on Seamless Transport   \n",
       "\n",
       "                                           objective summary workPerformed  \\\n",
       "0  Airport ground processes still conceal a consi...     NaN           NaN   \n",
       "\n",
       "  finalResults                                               text  \\\n",
       "0          NaN  ASSET – Aeronautic Study on Seamless Transport...   \n",
       "\n",
       "                                          clean_text     score framework  \n",
       "0  asset aeronautic study seamless transport airp...  0.572112       FP7  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDs shouldn't have changed from MR2 --> MR3\n",
    "\n",
    "mobility_projects[mobility_projects.id == 211625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rcn</th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>status</th>\n",
       "      <th>programme</th>\n",
       "      <th>topics</th>\n",
       "      <th>frameworkProgramme</th>\n",
       "      <th>title</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>...</th>\n",
       "      <th>coordinator</th>\n",
       "      <th>coordinatorCountry</th>\n",
       "      <th>participants</th>\n",
       "      <th>participantCountries</th>\n",
       "      <th>subjects</th>\n",
       "      <th>summary</th>\n",
       "      <th>lastUpdateDate</th>\n",
       "      <th>rcn_report</th>\n",
       "      <th>title_report</th>\n",
       "      <th>projectAcronym</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16245</th>\n",
       "      <td>90080</td>\n",
       "      <td>211625</td>\n",
       "      <td>ASSET</td>\n",
       "      <td>ONG</td>\n",
       "      <td>FP7-TRANSPORT</td>\n",
       "      <td>AAT-2007-2.2-02;AAT-2007-3.2-02</td>\n",
       "      <td>FP7</td>\n",
       "      <td>ASSET – Aeronautic Study on Seamless Transport</td>\n",
       "      <td>2008-06-01</td>\n",
       "      <td>2011-11-30</td>\n",
       "      <td>...</td>\n",
       "      <td>DEUTSCHES ZENTRUM FUER LUFT - UND RAUMFAHRT EV</td>\n",
       "      <td>DE</td>\n",
       "      <td>ID PARTNERS;IDEMIA IDENTITY &amp; SECURITY FRANCE;...</td>\n",
       "      <td>FR;EL;DE;UK;SK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rcn      id acronym status      programme  \\\n",
       "16245  90080  211625   ASSET    ONG  FP7-TRANSPORT   \n",
       "\n",
       "                                topics frameworkProgramme  \\\n",
       "16245  AAT-2007-2.2-02;AAT-2007-3.2-02                FP7   \n",
       "\n",
       "                                                title   startDate     endDate  \\\n",
       "16245  ASSET – Aeronautic Study on Seamless Transport  2008-06-01  2011-11-30   \n",
       "\n",
       "       ...                                     coordinator coordinatorCountry  \\\n",
       "16245  ...  DEUTSCHES ZENTRUM FUER LUFT - UND RAUMFAHRT EV                 DE   \n",
       "\n",
       "                                            participants participantCountries  \\\n",
       "16245  ID PARTNERS;IDEMIA IDENTITY & SECURITY FRANCE;...       FR;EL;DE;UK;SK   \n",
       "\n",
       "      subjects summary lastUpdateDate rcn_report title_report projectAcronym  \n",
       "16245      NaN     NaN            NaN        NaN          NaN            NaN  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp7[fp7.id == 211625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(926, 31)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate FP7 and H2020\n",
    "\n",
    "df_full = pd.concat([h2020, fp7], axis=0, join='outer', ignore_index=True)\n",
    "\n",
    "if not use_full_dataset:\n",
    "    # select just the mobilty projects\n",
    "    df_full = df_full[df_full.id.isin(mobility_projects_ids_list)]\n",
    "    \n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rcn</th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>status</th>\n",
       "      <th>programme</th>\n",
       "      <th>topics</th>\n",
       "      <th>frameworkProgramme</th>\n",
       "      <th>title</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>...</th>\n",
       "      <th>rcn_report</th>\n",
       "      <th>title_report</th>\n",
       "      <th>summary</th>\n",
       "      <th>workPerformed</th>\n",
       "      <th>finalResults</th>\n",
       "      <th>lastUpdateDate</th>\n",
       "      <th>projectID</th>\n",
       "      <th>projectAcronym</th>\n",
       "      <th>relatedFile</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>224495</td>\n",
       "      <td>876943</td>\n",
       "      <td>DAICY</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>H2020-EU.3.;H2020-EU.2.3.;H2020-EU.2.1.</td>\n",
       "      <td>EIC-SMEInst-2018-2020</td>\n",
       "      <td>H2020</td>\n",
       "      <td>Design and AI for sustainable and safe motorCY...</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>...</td>\n",
       "      <td>429360.0</td>\n",
       "      <td>Periodic Reporting for period 1 - DAICY (Desig...</td>\n",
       "      <td>Problems: \\nUrban driving safety \\nCO2 emissio...</td>\n",
       "      <td>We studied the European market. The most impor...</td>\n",
       "      <td>Expected result: Introduction of the safest, s...</td>\n",
       "      <td>2020-01-29 08:45:15</td>\n",
       "      <td>876943.0</td>\n",
       "      <td>DAICY</td>\n",
       "      <td>/docs/results/h2020/876/876943_PS/tarform-prof...</td>\n",
       "      <td>http://www.tarform.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>224539</td>\n",
       "      <td>878052</td>\n",
       "      <td>MAAS</td>\n",
       "      <td>CLOSED</td>\n",
       "      <td>H2020-EU.3.;H2020-EU.2.3.;H2020-EU.2.1.</td>\n",
       "      <td>EIC-SMEInst-2018-2020</td>\n",
       "      <td>H2020</td>\n",
       "      <td>MOBILITY AS A SERVICE PLATFORM for employers a...</td>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019-11-30</td>\n",
       "      <td>...</td>\n",
       "      <td>440380.0</td>\n",
       "      <td>Periodic Reporting for period 1 - MAAS (MOBILI...</td>\n",
       "      <td>Problems regarding urban transportation, such ...</td>\n",
       "      <td>Technical  feasibility – we have developed a r...</td>\n",
       "      <td>We have achieved a thorough analysis of the ta...</td>\n",
       "      <td>2020-03-20 19:09:32</td>\n",
       "      <td>878052.0</td>\n",
       "      <td>MAAS</td>\n",
       "      <td>/docs/results/h2020/878/878052_PS/maas.png</td>\n",
       "      <td>https://www.mobilityconcept.nl/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        rcn      id acronym  status                                programme  \\\n",
       "245  224495  876943   DAICY  CLOSED  H2020-EU.3.;H2020-EU.2.3.;H2020-EU.2.1.   \n",
       "335  224539  878052    MAAS  CLOSED  H2020-EU.3.;H2020-EU.2.3.;H2020-EU.2.1.   \n",
       "\n",
       "                    topics frameworkProgramme  \\\n",
       "245  EIC-SMEInst-2018-2020              H2020   \n",
       "335  EIC-SMEInst-2018-2020              H2020   \n",
       "\n",
       "                                                 title   startDate  \\\n",
       "245  Design and AI for sustainable and safe motorCY...  2019-07-01   \n",
       "335  MOBILITY AS A SERVICE PLATFORM for employers a...  2019-08-01   \n",
       "\n",
       "        endDate  ... rcn_report  \\\n",
       "245  2019-09-30  ...   429360.0   \n",
       "335  2019-11-30  ...   440380.0   \n",
       "\n",
       "                                          title_report  \\\n",
       "245  Periodic Reporting for period 1 - DAICY (Desig...   \n",
       "335  Periodic Reporting for period 1 - MAAS (MOBILI...   \n",
       "\n",
       "                                               summary  \\\n",
       "245  Problems: \\nUrban driving safety \\nCO2 emissio...   \n",
       "335  Problems regarding urban transportation, such ...   \n",
       "\n",
       "                                         workPerformed  \\\n",
       "245  We studied the European market. The most impor...   \n",
       "335  Technical  feasibility – we have developed a r...   \n",
       "\n",
       "                                          finalResults       lastUpdateDate  \\\n",
       "245  Expected result: Introduction of the safest, s...  2020-01-29 08:45:15   \n",
       "335  We have achieved a thorough analysis of the ta...  2020-03-20 19:09:32   \n",
       "\n",
       "    projectID projectAcronym  \\\n",
       "245  876943.0          DAICY   \n",
       "335  878052.0           MAAS   \n",
       "\n",
       "                                           relatedFile  \\\n",
       "245  /docs/results/h2020/876/876943_PS/tarform-prof...   \n",
       "335         /docs/results/h2020/878/878052_PS/maas.png   \n",
       "\n",
       "                                 url  \n",
       "245           http://www.tarform.com  \n",
       "335  https://www.mobilityconcept.nl/  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_full.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 31)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that all the projects ids are distinct\n",
    "\n",
    "df_full[df_full.duplicated(subset=['id'], keep=False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rcn', 'id', 'acronym', 'status', 'programme', 'topics',\n",
       "       'frameworkProgramme', 'title', 'startDate', 'endDate',\n",
       "       'projectUrl', 'objective', 'totalCost', 'ecMaxContribution',\n",
       "       'call', 'fundingScheme', 'coordinator', 'coordinatorCountry',\n",
       "       'participants', 'participantCountries', 'subjects', 'rcn_report',\n",
       "       'title_report', 'summary', 'workPerformed', 'finalResults',\n",
       "       'lastUpdateDate', 'projectID', 'projectAcronym', 'relatedFile',\n",
       "       'url'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract fields to be used in NLP\n",
    "\n",
    "- title, objective, summary, workPerformed, finalResults + other basic info (id, acronym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_full[['id', 'acronym', 'title', 'objective', 'summary', 'workPerformed', 'finalResults']].sort_values('id').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan with empty strings\n",
    "df.replace(np.nan, '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate textual (NL) data\n",
    "\n",
    "df['text'] = df[['title', 'objective', 'summary', 'workPerformed', 'finalResults']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "docs = list(df['text'].values)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.text[0]) # THIS IS WHAT IS CONSIDERED THE DOCUMENT FOR THE NLP POV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[df.shape[0]-1,'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.loc[2,'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda64e75ddc4576af0328380df32050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=926.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# break each doc into sentences\n",
    "df['sentences'] = df.text.progress_map(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nanoelectronics for Safe, Fuel Efficient and Environment Friendly Automotive Solutions The societal need for a transport infrastructure based upon the availability of safe, fuel-efficient and environmental-friendly cars is clearly recognized by the European citizens and the European Commission.',\n",
       " 'The fulfillment of this ambition is not to be taken for granted, as it requires the development of a host of  automotive technologies, systems, software and tools.',\n",
       " 'It is the objective of this project to create an integrated automotive control platform, enabled by breakthroughs in the areas of efficient fuel consumption, reduced CO2 emission and safe driving, to be achieved by the development of nanoelectronic components, subsystems and architectures.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentences[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59011a16d0b4f8ead81ef3491a1f0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=926.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# INTO TOKENS\n",
    "\n",
    "df['tokens'] = df['sentences'].progress_map(lambda sentences: [word_tokenize(sentence) \\\n",
    "                                                                   for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['tokens'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization with POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896143c2f6994f20abb91d05177e61ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=926.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['pos_tokens'] = df['tokens'].progress_map(lambda tokens: [pos_tag(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Nanoelectronics', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('Safe', 'NNP'),\n",
       " (',', ','),\n",
       " ('Fuel', 'NNP'),\n",
       " ('Efficient', 'NNP')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pos_tokens[0][0][:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from https://stackoverflow.com/a/15590384\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2678b3c194c4d81ad12915dcd499c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=926.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing each word with its POS tag: that way, all the verbs are lemmatize to infinitive,\n",
    "# all the nouns to their dictionary forms etc.\n",
    "\n",
    "\n",
    "df['lemmas'] = df['pos_tokens'].progress_map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1])) \n",
    "            if get_wordnet_pos(el[1]) != '' \n",
    "            else el[0] for el in pos_tokens  # each el in pos_tokens is a tuple of a token and their POS tag\n",
    "        ] \n",
    "        for pos_tokens in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nanoelectronics',\n",
       " 'for',\n",
       " 'Safe',\n",
       " ',',\n",
       " 'Fuel',\n",
       " 'Efficient',\n",
       " 'and',\n",
       " 'Environment']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.lemmas[0][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', \\\n",
    "                   'use', 'would', 'can', 'show', 'think', 'deduce', 'prove', 'experiment']\n",
    "stopwords_research = ['project', 'research', 'initiative', 'h2020', 'fp7', 'science', 'investigation']\n",
    "\n",
    "stopwords_other = [\"also\", \"develop\", \"one\", \"new\", \"include\", \"well\", \"work\", \"provide\", \"approach\", \\\n",
    "                   \"different\", \"time\", \"good\", \"result\", 'activity', 'development', 'key', 'analysis', \\\n",
    "                  'impact', 'tool', 'develop', 'method', 'level', 'task', 'change', 'large', 'objective', \\\n",
    "                  'perform', 'time', 'good', 'increase', 'process']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_stopwords = stopwords.words('English') + stopwords_verbs + stopwords_research + stopwords_other\n",
    "len(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain # to flatten list of sentences of tokens into list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['lemmas'].map(lambda sentences: list(chain.from_iterable(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nanoelectronics',\n",
       " 'for',\n",
       " 'Safe',\n",
       " ',',\n",
       " 'Fuel',\n",
       " 'Efficient',\n",
       " 'and',\n",
       " 'Environment']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pre-defined stopwords, tokens of length 1 and any tokens that are not alphabetic\n",
    "df['tokens'] = df['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha()\n",
    "                                                   and\n",
    "                                                   token.lower() not in my_stopwords\n",
    "                                                   and len(token) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nanoelectronics',\n",
       " 'safe',\n",
       " 'fuel',\n",
       " 'efficient',\n",
       " 'environment',\n",
       " 'friendly',\n",
       " 'automotive',\n",
       " 'solutions']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'][0][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [nanoelectronics, safe, fuel, efficient, envir...\n",
       "1    [flight, dynamic, control, bird, insect, insec...\n",
       "2    [acare, goals, progress, evaluation, acare, ad...\n",
       "3    [strengthening, railway, vehicles, center, fac...\n",
       "4    [clean, sky, support, action, sky, joint, tech...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "926"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nanoelectronics safe fuel efficient environment friendly automotive solutions societal transport infrastructure base upon availability safe car clearly recognize european citizen european commission fulfillment ambition grant require host automotive technology system software create integrated automotive control platform enable breakthrough area efficient fuel consumption reduce emission safe driving achieve nanoelectronic component subsystem architecture goal line target define eniac strategic agenda target parameter following reduction annual cost cause road accident estimate europe improvement fuel consumption efficiency reduction emission line reduction plan subdivide following workpackages definition carrier specification design optimized control system novel automotive technology integration sub system design reliability test yield prototyping functional verification objectives dissemination release deliverables execute consortium incorporate major european industrial institutional academic player field innovation automotive application partner locate european company constitute globally competitive team best qualification job'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(df.tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create one big string for each project's cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df[\"tokens\"].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.clean_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>acronym</th>\n",
       "      <th>title</th>\n",
       "      <th>objective</th>\n",
       "      <th>summary</th>\n",
       "      <th>workPerformed</th>\n",
       "      <th>finalResults</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120009</td>\n",
       "      <td>SE2A</td>\n",
       "      <td>Nanoelectronics for Safe, Fuel Efficient and E...</td>\n",
       "      <td>The societal need for a transport infrastructu...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Nanoelectronics for Safe, Fuel Efficient and E...</td>\n",
       "      <td>[Nanoelectronics for Safe, Fuel Efficient and ...</td>\n",
       "      <td>[nanoelectronics, safe, fuel, efficient, envir...</td>\n",
       "      <td>[[(Nanoelectronics, NNS), (for, IN), (Safe, NN...</td>\n",
       "      <td>[[Nanoelectronics, for, Safe, ,, Fuel, Efficie...</td>\n",
       "      <td>nanoelectronics safe fuel efficient environmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204513</td>\n",
       "      <td>DCBIF</td>\n",
       "      <td>Flight dynamics and control of birds and insects</td>\n",
       "      <td>Insects bristle with sensors, but how do they ...</td>\n",
       "      <td>This project aimed to develop an understanding...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Flight dynamics and control of birds and insec...</td>\n",
       "      <td>[Flight dynamics and control of birds and inse...</td>\n",
       "      <td>[flight, dynamic, control, bird, insect, insec...</td>\n",
       "      <td>[[(Flight, NNP), (dynamics, NNS), (and, CC), (...</td>\n",
       "      <td>[[Flight, dynamic, and, control, of, bird, and...</td>\n",
       "      <td>flight dynamic control bird insect insects bri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id acronym                                              title  \\\n",
       "0  120009    SE2A  Nanoelectronics for Safe, Fuel Efficient and E...   \n",
       "1  204513   DCBIF   Flight dynamics and control of birds and insects   \n",
       "\n",
       "                                           objective  \\\n",
       "0  The societal need for a transport infrastructu...   \n",
       "1  Insects bristle with sensors, but how do they ...   \n",
       "\n",
       "                                             summary workPerformed  \\\n",
       "0                                                                    \n",
       "1  This project aimed to develop an understanding...                 \n",
       "\n",
       "  finalResults                                               text  \\\n",
       "0               Nanoelectronics for Safe, Fuel Efficient and E...   \n",
       "1               Flight dynamics and control of birds and insec...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [Nanoelectronics for Safe, Fuel Efficient and ...   \n",
       "1  [Flight dynamics and control of birds and inse...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [nanoelectronics, safe, fuel, efficient, envir...   \n",
       "1  [flight, dynamic, control, bird, insect, insec...   \n",
       "\n",
       "                                          pos_tokens  \\\n",
       "0  [[(Nanoelectronics, NNS), (for, IN), (Safe, NN...   \n",
       "1  [[(Flight, NNP), (dynamics, NNS), (and, CC), (...   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  [[Nanoelectronics, for, Safe, ,, Fuel, Efficie...   \n",
       "1  [[Flight, dynamic, and, control, of, bird, and...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  nanoelectronics safe fuel efficient environmen...  \n",
       "1  flight dynamic control bird insect insects bri...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as it is to CSVs - careful, lists are converted to strings\n",
    "\n",
    "df.to_csv(\"datasets/data_may20/outputs/mobility_projects_clean_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to pickle as well, so that lists can be loaded back as lists\n",
    "\n",
    "df.to_pickle(\"datasets/data_may20/outputs/mobility_projects_clean_text.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to parquet - DO IT LATER\n",
    "\n",
    "#DataFrame.to_parquet(self, fname, engine='auto', compression='snappy', index=None, partition_cols=None, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset from the save CSV (clean_text column is important)\n",
    "\n",
    "#df = pd.read_csv(\"datasets/data_oct19/auxiliar/full_dataset_clean_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.clean_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vocabulary\n",
    "\n",
    "Using the tokenised and clean texts, create a full vocabulary to be used by ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_construction_old(text, remove_singletons = True):\n",
    "    \"\"\"\n",
    "    Old function, left for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    voc = []\n",
    "    i = 0\n",
    "    \n",
    "    for txt in text:\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Iteration \", i)\n",
    "        i += 1\n",
    "            \n",
    "        # separate into sentences\n",
    "        #sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "        # split into words\n",
    "        tokens = nltk.word_tokenize(txt)\n",
    "\n",
    "        # keep only word tokens (words), and make them all lowercase\n",
    "        words = [w.lower() for w in tokens if w.isalpha()]\n",
    "\n",
    "        # exclude stopwords in english\n",
    "        filtered_words = [w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "        # stem the words using PorterStemmer (nltk)\n",
    "        porter = nltk.stem.PorterStemmer()\n",
    "        stemmed = [porter.stem(w) for w in filtered_words]\n",
    "\n",
    "        # lemmatize with WordNetLemmatizer\n",
    "        wnl = nltk.WordNetLemmatizer()\n",
    "        lemmas = [wnl.lemmatize(w) for w in stemmed]\n",
    "        \n",
    "        voc += lemmas\n",
    "        #print(voc)\n",
    "\n",
    "    dist = nltk.probability.FreqDist(voc) # dist is of type nltk.probability.FreqDist- contains a dict.\n",
    "        \n",
    "    \n",
    "    # if remove_singletons is set to True, it will remove from the vocabulary all the words that appear only once\n",
    "    if remove_singletons:\n",
    "        singletons = [w for w in dist.keys() if dist[w] == 1]\n",
    "        voc = set(voc) - set(singletons)\n",
    "    \n",
    "    return set(voc), dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_construction(token_list, remove_singletons=False):\n",
    "    \"\"\"\n",
    "    Using field 'tokens' in the dataframe df, construct a FreqDist,\n",
    "    as well as just a regular set that stands for a vocabulary.\n",
    "    \"\"\"\n",
    "    voc = list(itertools.chain.from_iterable(token_list))\n",
    "        \n",
    "    dist = nltk.probability.FreqDist(voc) # dist is of type nltk.probability.FreqDist- contains a dict.\n",
    "    \n",
    "    # if remove_singletons is set to True, it will remove from the vocabulary all the words that appear only once\n",
    "    if remove_singletons:\n",
    "        singletons = [w for w in dist.keys() if dist[w] == 1]\n",
    "        voc = set(voc) - set(singletons)\n",
    "        \n",
    "    return dist, set(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_construction_from_text(clean_text_list, remove_singletons=False):\n",
    "    \"\"\"\n",
    "    For each project, clean text is constructed from the tokens takens from that text\n",
    "    and concatenated into one string.\n",
    "    \"\"\"\n",
    "    voc = [w for segm in clean_text_list for w in segm.split()]\n",
    "        \n",
    "    dist = nltk.probability.FreqDist(voc) # dist is of type nltk.probability.FreqDist- contains a dict.\n",
    "    \n",
    "    # if remove_singletons is set to True, it will remove from the vocabulary all the words that appear only once\n",
    "    if remove_singletons:\n",
    "        singletons = [w for w in dist.keys() if dist[w] == 1]\n",
    "        voc = set(voc) - set(singletons)\n",
    "        \n",
    "    return dist, set(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test vocabulary_construction\n",
    "exmpl_text = [\"My little cat went to little wood called Wood and met Little dog.\", \\\n",
    "              \" It was raining cats that day.\"]\n",
    "\n",
    "exmpl_text2 = ['little cat wood dog',\n",
    "              'rain cat day dog',\n",
    "              'dog wood']\n",
    "\n",
    "exmpl_tokens = [['little', 'cat', 'wood', 'dog'],\n",
    "                ['rain', 'cat', 'day', 'dog'],\n",
    "                ['dog','wood']\n",
    "    ]\n",
    "\n",
    "#dist_exmpl, dist1 = vocabulary_construction_old(exmpl_text)\n",
    "\n",
    "#dist_exmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat', 'day', 'dog', 'little', 'rain', 'wood'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist, voc2 = vocabulary_construction(exmpl_tokens)\n",
    "print(len(voc2))\n",
    "voc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat', 'day', 'dog', 'little', 'rain', 'wood'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist, voc2 = vocabulary_construction_from_text(exmpl_text2)\n",
    "print(len(voc2))\n",
    "voc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat', 'dog', 'wood'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist, voc2 = vocabulary_construction(exmpl_tokens, remove_singletons=True)\n",
    "print(len(voc2))\n",
    "voc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat', 'dog', 'wood'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist, voc2 = vocabulary_construction_from_text(exmpl_text2, remove_singletons=True)\n",
    "print(len(voc2))\n",
    "voc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'dog': 3, 'cat': 2, 'wood': 2, 'little': 1, 'rain': 1, 'day': 1})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vocabulary using all the projects\n",
    "execute_vocabulary_construction = True\n",
    "\n",
    "if execute_vocabulary_construction:\n",
    "    freqdist_full, vocabulary = vocabulary_construction(list(df.tokens), remove_singletons=True)\n",
    "    len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "voc = list(vocabulary)\n",
    "\n",
    "save_vocabulary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quasi', 'ana', 'intensity', 'registered', 'homogenize', 'depart', 'tolling', 'fcb', 'presently', 'wrp', 'reichel', 'prototyping', 'kom', 'arrow', 'personalised', 'resonator', 'poles', 'piece', 'cumulated', 'syndrome']\n"
     ]
    }
   ],
   "source": [
    "if save_vocabulary:\n",
    "    print(voc[0:20])\n",
    "\n",
    "    with open('datasets/data_may20/outputs/vocabulary_mobility_projects_nosingletons.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(voc, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15023\n"
     ]
    }
   ],
   "source": [
    "# read the vocabulary\n",
    "with open('datasets/data_may20/outputs/vocabulary_mobility_projects_nosingletons.json', 'r') as f:\n",
    "        voc_read = json.load(f)\n",
    "\n",
    "print(type(voc_read), len(voc_read))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quasi', 'ana', 'intensity', 'registered', 'homogenize', 'depart', 'tolling', 'fcb', 'presently', 'wrp', 'reichel', 'prototyping', 'kom', 'arrow', 'personalised', 'resonator', 'poles', 'piece', 'cumulated', 'syndrome']\n"
     ]
    }
   ],
   "source": [
    "print(voc_read[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project   False\n",
      "mumba_bumba   False\n",
      "mobility   True\n",
      "h2020   False\n",
      "transport   True\n",
      "krebs   False\n",
      "10   False\n",
      "air   True\n",
      "aircraft   True\n",
      "traffic   True\n",
      "aviation   True\n",
      "passenger   True\n",
      "flight   True\n",
      "transportation   True\n",
      "airline   True\n",
      "mobil   True\n",
      "aviat   False\n",
      "airlin   False\n",
      "airport   True\n",
      "research   False\n",
      "innovation   True\n"
     ]
    }
   ],
   "source": [
    "# some SANITY CHECKS\n",
    "\n",
    "words_sanity = [\"project\", \"mumba_bumba\", \"mobility\", \"h2020\", \"transport\", \"krebs\", \"10\", \"air\", \"aircraft\", \\\n",
    "               \"traffic\", \"aviation\", \"passenger\", \"flight\", \"transportation\", \"airline\", \"mobil\", \\\n",
    "               \"aviat\", \"airlin\", \"airport\", \"research\", \"innovation\"]\n",
    "\n",
    "for w in words_sanity:\n",
    "    print(w, \" \",w in voc_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intermodality   True\n",
      "intermodal   True\n",
      "crossmodality   False\n",
      "crossmodal   True\n",
      "comodal   False\n",
      "comodality   True\n",
      "also   False\n",
      "one   False\n",
      "approach   False\n"
     ]
    }
   ],
   "source": [
    "# check some extra words\n",
    "\n",
    "words_extra = [\"intermodality\", \"intermodal\", \"crossmodality\", \"crossmodal\", \"comodal\", \"comodality\", \"also\", \\\n",
    "               \"one\", \"approach\"]\n",
    "\n",
    "for w in words_extra:\n",
    "    print(w, \" \",w in voc_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer by sklearn\n",
    "\n",
    "Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and trigrams\n",
    "\n",
    "\n",
    "# TO DO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['little cat wood dog', 'rain cat day dog cat', 'dog wood']\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cat', 'day', 'dog', 'little', 'rain', 'wood'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test vocabulary_construction\n",
    "exmpl_text = [\"My little cat went to little wood called Wood and met Little dog.\", \\\n",
    "              \" It was raining cats that day.\"]\n",
    "\n",
    "exmpl_tokens = [['little', 'cat', 'wood', 'dog'],\n",
    "                ['rain', 'cat', 'day', 'dog', 'cat'],\n",
    "                ['dog','wood']\n",
    "    ]\n",
    "\n",
    "exmpl_docs = [\" \".join(x) for x in exmpl_tokens]\n",
    "print(exmpl_docs)\n",
    "\n",
    "dist, voc2 = vocabulary_construction(exmpl_tokens, remove_singletons = False)\n",
    "print(len(voc2))\n",
    "voc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams\n",
    "from nltk.util import everygrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat day',\n",
       " 'cat day dog',\n",
       " 'cat wood',\n",
       " 'cat wood dog',\n",
       " 'day dog',\n",
       " 'day dog cat',\n",
       " 'dog cat',\n",
       " 'dog wood',\n",
       " 'little cat',\n",
       " 'little cat wood',\n",
       " 'rain cat',\n",
       " 'rain cat day',\n",
       " 'wood dog'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_trigram_list = []\n",
    "\n",
    "for x in exmpl_tokens:\n",
    "    bigram_trigram_list += list(everygrams(x, min_len=2, max_len=3))\n",
    "\n",
    "# CREATE A GENERATOR!\n",
    "allgram_generator = (\" \".join(x) for x in bigram_trigram_list)\n",
    "\n",
    "set(allgram_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little cat, cat wood, wood dog, little cat wood, cat wood dog, rain cat, cat day, day dog, dog cat, rain cat day, cat day dog, day dog cat, dog wood\n"
     ]
    }
   ],
   "source": [
    "print(*map(' '.join, bigram_trigram_list), sep=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with building of vocabulary\n",
    "\"\"\"\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1,\n",
    "                                max_features=10000,\n",
    "                                stop_words='english')\n",
    "                                \"\"\"\n",
    "\n",
    "#use pre-built vocabulary\n",
    "\n",
    "tf_vectorizer = CountVectorizer(vocabulary= voc2)\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(exmpl_docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 19)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'cat day',\n",
       " 'cat day dog',\n",
       " 'cat wood',\n",
       " 'cat wood dog',\n",
       " 'day',\n",
       " 'day dog',\n",
       " 'day dog cat',\n",
       " 'dog',\n",
       " 'dog cat',\n",
       " 'dog wood',\n",
       " 'little',\n",
       " 'little cat',\n",
       " 'little cat wood',\n",
       " 'rain',\n",
       " 'rain cat',\n",
       " 'rain cat day',\n",
       " 'wood',\n",
       " 'wood dog']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer by sklearn\n",
    "\n",
    "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "Equivalent to CountVectorizer followed by TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = TfidfVectorizer(vocabulary=set(voc2))\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(exmpl_docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4804584 , 0.        , 0.37311881, 0.63174505, 0.        ,\n",
       "        0.4804584 ],\n",
       "       [0.70443024, 0.46312056, 0.27352646, 0.        , 0.46312056,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.61335554, 0.        , 0.        ,\n",
       "        0.78980693]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.shape)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48,  0, 37, 63,  0, 48],\n",
       "       [70, 46, 27,  0, 46,  0],\n",
       "       [ 0,  0, 61,  0,  0, 79]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "For guided LDA to work, this needs to be converted into a matrix of integer.\n",
    "\"\"\"\n",
    "\n",
    "tf2 = np.rint(tf * 100).astype(int)\n",
    "tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51235, 208181)\n"
     ]
    }
   ],
   "source": [
    "# inspect the values that TF-IDF gives when working with \n",
    "docs = list(df['clean_text'].values)\n",
    "tf_vectorizer = TfidfVectorizer(vocabulary = set(voc_read))\n",
    "            \n",
    "tf = tf_vectorizer.fit_transform(docs)\n",
    "\n",
    "print(tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf2 = np.rint(tf * 100).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#voc_final = voc_read\n",
    "voc_final = voc\n",
    "type(voc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208153"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files = os.listdir(\"mr2_results/lda_outputs/\")\n",
    "\n",
    "files_lda = glob.glob(\"mr2_results/lda_outputs/*.csv\")\n",
    "\n",
    "# just v2.X files\n",
    "files_lda = glob.glob(\"mr2_results/lda_outputs/*\" + version + \".csv\")\n",
    "\n",
    "len(files_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
